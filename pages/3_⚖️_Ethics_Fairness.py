"""
ä¼¦ç†ä¸å…¬å¹³é¡µé¢
å±•ç¤ºä¼¦ç†åŸåˆ™å’Œåè§ç¼“è§£ç­–ç•¥
"""

import streamlit as st
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
import numpy as np
from config.settings import CUSTOM_CSS, FAIRNESS_THRESHOLDS, DEMOGRAPHIC_GROUPS

st.set_page_config(page_title="Ethics & Fairness", page_icon="âš–ï¸", layout="wide")
st.markdown(CUSTOM_CSS, unsafe_allow_html=True)

def main():
    st.title("âš–ï¸ Ethics & Fairness")
    st.markdown("### Ethical Principles & Bias Mitigation")
    
    tab1, tab2, tab3, tab4 = st.tabs([
        "ğŸ¯ Ethical Principles",
        "âš ï¸ Bias Sources",
        "ğŸ› ï¸ Mitigation Strategies",
        "ğŸ“Š Fairness Metrics"
    ])
    
    with tab1:
        show_ethical_principles()
    
    with tab2:
        show_bias_sources()
    
    with tab3:
        show_mitigation_strategies()
    
    with tab4:
        show_fairness_metrics()

def show_ethical_principles():
    """å±•ç¤ºä¼¦ç†åŸåˆ™"""
    st.markdown("## Core Ethical Principles")
    
    principles = {
        "Non-maleficence": {
            "icon": "ğŸ›¡ï¸",
            "definition": "First, do no harm",
            "application": [
                "Minimize false negatives (missed high-risk cases)",
                "Threshold selection prioritizes survivor safety",
                "Impact assessment before deployment",
                "Continuous harm monitoring"
            ]
        },
        "Justice & Fairness": {
            "icon": "âš–ï¸",
            "definition": "Equal treatment and outcomes across groups",
            "application": [
                "Subgroup-specific error rate monitoring",
                "Bias mitigation for marginalized populations",
                "Equal opportunity constraints in training",
                "Regular fairness audits"
            ]
        },
        "Transparency": {
            "icon": "ğŸ’¡",
            "definition": "Explainable and understandable decisions",
            "application": [
                "SHAP-based feature importance",
                "Attention mechanism visualization",
                "Clear documentation of model logic",
                "Accessible explanations for practitioners"
            ]
        },
        "Accountability": {
            "icon": "ğŸ“‹",
            "definition": "Clear responsibility for decisions and outcomes",
            "application": [
                "Immutable audit logs",
                "Decision traceability",
                "Defined escalation procedures",
                "Regular system audits"
            ]
        },
        "Human Oversight": {
            "icon": "ğŸ‘ï¸",
            "definition": "Humans make final high-stakes decisions",
            "application": [
                "Mandatory review for high-risk predictions",
                "Override capability for trained professionals",
                "AI as decision support, not decision maker",
                "Training for system users"
            ]
        },
        "Dignity & Autonomy": {
            "icon": "ğŸ¤",
            "definition": "Respect for individual rights and choices",
            "application": [
                "Survivor-centered governance",
                "Privacy protection paramount",
                "No automated detention/removal",
                "Support, not surveillance"
            ]
        }
    }
    
    cols = st.columns(2)
    
    for idx, (principle, details) in enumerate(principles.items()):
        with cols[idx % 2]:
            with st.expander(f"{details['icon']} {principle}"):
                st.markdown(f"**Definition**: {details['definition']}")
                st.markdown("**Our Implementation**:")
                for app in details['application']:
                    st.markdown(f"âœ“ {app}")
    
    st.markdown("---")
    st.markdown("### Error Cost Matrix")
    
    st.markdown("""
    <div class="warning-box">
    <h4>âš ï¸ Asymmetric Costs</h4>
    <p>In DV risk prediction, false negatives (missing high-risk cases) have <strong>higher ethical costs</strong> 
    than false positives (over-predicting risk), as they can lead to preventable harm or death.</p>
    </div>
    """, unsafe_allow_html=True)
    
    # æˆæœ¬çŸ©é˜µ
    cost_matrix = pd.DataFrame({
        'True Positive': ['âœ“ Correct identification', 'Timely intervention', 'Lives saved'],
        'False Positive': ['âœ— Over-prediction', 'Unnecessary interventions', 'Moderate cost'],
        'True Negative': ['âœ“ Correct non-risk', 'No intervention needed', 'Resources saved'],
        'False Negative': ['âœ—âœ— Missed high-risk', 'No intervention', 'SEVERE COST: Potential harm/death']
    }, index=['Outcome', 'Action', 'Cost'])
    
    st.table(cost_matrix)
    
    st.markdown("""
    **Model Optimization**: We use a **cost-sensitive loss function** that penalizes 
    false negatives more heavily than false positives, reflecting their real-world consequences.
    """)

def show_bias_sources():
    """å±•ç¤ºåè§æ¥æº"""
    st.markdown("## Bias Sources in DV Data")
    
    st.markdown("""
    <div class="info-box">
    <h4>ğŸ“Œ Why Bias Occurs</h4>
    <p>DV datasets reflect systemic inequalities in reporting, access to services, and data collection practices.</p>
    </div>
    """, unsafe_allow_html=True)
    
    bias_types = {
        "Under-reporting Bias": {
            "description": "Women from certain groups are less likely to report DV",
            "affected_groups": ["Migrant communities", "Rural areas", "Low socioeconomic status"],
            "causes": ["Cultural stigma", "Language barriers", "Distrust of authorities", "Limited service access"],
            "impact": "Systematic underrepresentation in training data â†’ Higher false negative rates"
        },
        "Measurement Bias": {
            "description": "Inconsistent recording of DV incidents across institutions",
            "affected_groups": ["Communities with poor healthcare access", "Regions with inconsistent police practices"],
            "causes": ["Lack of standardized protocols", "Varying documentation quality", "Implicit biases in recording"],
            "impact": "Noisy labels â†’ Model learns incorrect patterns"
        },
        "Historical Bias": {
            "description": "Past discrimination reflected in historical data",
            "affected_groups": ["Ethnic minorities", "Indigenous communities", "LGBTQ+ individuals"],
            "causes": ["Discriminatory policies", "Biased risk assessment tools", "Systemic inequalities"],
            "impact": "Model perpetuates historical injustices"
        },
        "Selection Bias": {
            "description": "Data collection focuses on accessible populations",
            "affected_groups": ["Remote rural areas", "Undocumented immigrants", "Homeless women"],
            "causes": ["Sampling convenience", "Resource constraints", "Geographic limitations"],
            "impact": "Poor generalization to underrepresented groups"
        }
    }
    
    for bias_name, details in bias_types.items():
        with st.expander(f"âš ï¸ {bias_name}"):
            st.markdown(f"**Description**: {details['description']}")
            st.markdown(f"**Affected Groups**: {', '.join(details['affected_groups'])}")
            st.markdown(f"**Causes**:")
            for cause in details['causes']:
                st.markdown(f"- {cause}")
            st.markdown(f"**Impact on Model**: {details['impact']}")
    
    # å¯è§†åŒ–åè§å½±å“
    st.markdown("---")
    st.markdown("### Bias Impact Simulation")
    
    # æ¨¡æ‹Ÿä¸åŒç¾¤ä½“çš„å‡é˜´æ€§ç‡
    groups = ['Majority', 'Migrant', 'Rural', 'Low SES']
    baseline_fnr = [0.10, 0.25, 0.22, 0.20]
    mitigated_fnr = [0.11, 0.13, 0.14, 0.12]
    
    fig = go.Figure()
    fig.add_trace(go.Bar(
        x=groups,
        y=baseline_fnr,
        name='Without Bias Mitigation',
        marker_color='red'
    ))
    fig.add_trace(go.Bar(
        x=groups,
        y=mitigated_fnr,
        name='With Bias Mitigation',
        marker_color='green'
    ))
    
    fig.update_layout(
        title="False Negative Rates Across Groups",
        yaxis_title="False Negative Rate",
        barmode='group',
        height=400
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    st.markdown("""
    **Key Insight**: Without mitigation, vulnerable groups have 2-2.5Ã— higher false negative rates, 
    meaning they are systematically under-protected by the model.
    """)

def show_mitigation_strategies():
    """å±•ç¤ºç¼“è§£ç­–ç•¥"""
    st.markdown("## Three-Stage Bias Mitigation Pipeline")
    
    stages = {
        "Stage 1: Pre-processing": {
            "color": "#2196F3",
            "techniques": [
                {
                    "name": "Reweighing",
                    "description": "Assign higher weights to underrepresented groups during training",
                    "code": """
from aif360.algorithms.preprocessing import Reweighing

# è®¡ç®—å®ä¾‹æƒé‡
rw = Reweighing(unprivileged_groups=unprivileged,
                privileged_groups=privileged)
dataset_transformed = rw.fit_transform(dataset)

# æƒé‡åº”ç”¨äºæŸå¤±å‡½æ•°
loss = weighted_loss(predictions, targets, weights=dataset_transformed.instance_weights)
                    """
                },
                {
                    "name": "SMOTE Oversampling",
                    "description": "Synthesize new samples for minority groups using k-nearest neighbors",
                    "code": """
from imblearn.over_sampling import SMOTE

# å¯¹å°‘æ•°ç¾¤ä½“è¿‡é‡‡æ ·
smote = SMOTE(sampling_strategy='minority', k_neighbors=5)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

print(f"Original: {Counter(y_train)}")
print(f"Resampled: {Counter(y_resampled)}")
                    """
                },
                {
                    "name": "Targeted Imputation",
                    "description": "Use group-specific imputation for missing data",
                    "code": """
from sklearn.impute import KNNImputer

# åˆ†ç»„å¡«å……ç¼ºå¤±å€¼
for group in ['migrant', 'rural', 'urban']:
    imputer = KNNImputer(n_neighbors=5)
    group_data = data[data['group'] == group]
    data.loc[data['group'] == group] = imputer.fit_transform(group_data)
                    """
                }
            ]
        },
        "Stage 2: In-processing": {
            "color": "#4CAF50",
            "techniques": [
                {
                    "name": "Fair Loss Optimization",
                    "description": "Add fairness constraints to the loss function",
                    "code": """
# å…¬å¹³æ€§æ„ŸçŸ¥æŸå¤±
def fair_loss(predictions, targets, sensitive_attr):
    # æ ‡å‡†äº¤å‰ç†µæŸå¤±
    ce_loss = CrossEntropyLoss(predictions, targets)
    
    # ç­‰æœºä¼šçº¦æŸ: min(TPR_disparity)
    tpr_disparity = equalized_odds_loss(predictions, targets, sensitive_attr)
    
    # ç»„åˆæŸå¤±
    total_loss = ce_loss + lambda_fairness * tpr_disparity
    return total_loss
                    """
                },
                {
                    "name": "Adversarial Debiasing",
                    "description": "Train adversary to remove bias from learned representations",
                    "code": """
# ä¸»åˆ†ç±»å™¨
predictor = MainClassifier()

# å¯¹æŠ—å™¨è¯•å›¾é¢„æµ‹æ•æ„Ÿå±æ€§
adversary = AdversaryNetwork()

# è”åˆè®­ç»ƒ
pred_loss = classification_loss(predictor(x), y)
adv_loss = adversary_loss(adversary(predictor.hidden), sensitive)

# æœ€å°åŒ–é¢„æµ‹æŸå¤±,æœ€å¤§åŒ–å¯¹æŠ—æŸå¤±
total_loss = pred_loss - lambda_adv * adv_loss
                    """
                }
            ]
        },
        "Stage 3: Post-processing": {
            "color": "#FF9800",
            "techniques": [
                {
                    "name": "Group-specific Threshold Calibration",
                    "description": "Optimize decision thresholds separately for each group",
                    "code": """
from sklearn.calibration import CalibratedClassifierCV

# ä¸ºæ¯ä¸ªç¾¤ä½“æ ¡å‡†é˜ˆå€¼
calibrated_models = {}
for group in demographic_groups:
    group_data = data[data['group'] == group]
    calibrator = CalibratedClassifierCV(base_model, method='isotonic')
    calibrator.fit(group_data.X, group_data.y)
    calibrated_models[group] = calibrator

# é¢„æµ‹æ—¶ä½¿ç”¨å¯¹åº”ç¾¤ä½“çš„æ ¡å‡†æ¨¡å‹
def predict(x, group):
    return calibrated_models[group].predict_proba(x)
                    """
                },
                {
                    "name": "Equalized Odds Post-processing",
                    "description": "Adjust predictions to achieve equal TPR and FPR across groups",
                    "code": """
from aif360.algorithms.postprocessing import EqOddsPostprocessing

# åå¤„ç†ä¼˜åŒ–
eq_odds = EqOddsPostprocessing(
    unprivileged_groups=unprivileged,
    privileged_groups=privileged
)

# åœ¨éªŒè¯é›†ä¸Šå­¦ä¹ è½¬æ¢
eq_odds.fit(dataset_valid, dataset_pred)

# åº”ç”¨åˆ°æµ‹è¯•é›†
dataset_transformed = eq_odds.predict(dataset_test)
                    """
                }
            ]
        }
    }
    
    for stage, details in stages.items():
        st.markdown(f"### {stage}")
        
        for tech in details['techniques']:
            with st.expander(f"ğŸ› ï¸ {tech['name']}"):
                st.markdown(tech['description'])
                st.code(tech['code'], language="python")

def show_fairness_metrics():
    """å±•ç¤ºå…¬å¹³æ€§æŒ‡æ ‡"""
    st.markdown("## Fairness Evaluation Metrics")
    
    # æŒ‡æ ‡å®šä¹‰
    metrics = {
        "False Positive Rate (FPR)": "Proportion of actual negatives incorrectly classified as positive",
        "False Negative Rate (FNR)": "Proportion of actual positives incorrectly classified as negative",
        "Equal Opportunity": "TPR should be equal across groups",
        "Equalized Odds": "Both TPR and FPR should be equal across groups",
        "Demographic Parity": "Positive prediction rate should be equal across groups",
        "Predictive Parity": "PPV should be equal across groups"
    }
    
    for metric, definition in metrics.items():
        st.markdown(f"**{metric}**: {definition}")
    
    st.markdown("---")
    
    # æ¨¡æ‹Ÿå…¬å¹³æ€§è¯„ä¼°ç»“æœ
    st.markdown("### Interactive Fairness Dashboard")
    
    col1, col2 = st.columns(2)
    
    with col1:
        selected_metric = st.selectbox(
            "Select Fairness Metric",
            ["False Negative Rate", "False Positive Rate", "True Positive Rate"]
        )
    
    with col2:
        selected_groups = st.multiselect(
            "Select Demographic Groups",
            DEMOGRAPHIC_GROUPS,
            default=["Age Group", "Migration Status"]
        )
    
    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    if selected_groups:
        data = []
        for group in selected_groups:
            subgroups = {
                "Age Group": ["18-30", "31-45", "46-60", "60+"],
                "Migration Status": ["Native", "Migrant", "Refugee"],
                "Socioeconomic Status": ["High", "Medium", "Low"],
                "Education Level": ["University", "High School", "Primary"],
                "Parental Status": ["Parents", "Non-parents"],
                "Rural/Urban": ["Urban", "Rural"]
            }
            
            for subgroup in subgroups.get(group, []):
                baseline = np.random.uniform(0.15, 0.25) if selected_metric == "False Negative Rate" else np.random.uniform(0.05, 0.15)
                data.append({
                    "Group": group,
                    "Subgroup": subgroup,
                    "Baseline Model": baseline,
                    "Fair Model": baseline * np.random.uniform(0.6, 0.8)
                })
        
        df = pd.DataFrame(data)
        
        fig = px.bar(
            df,
            x="Subgroup",
            y=["Baseline Model", "Fair Model"],
            color_discrete_sequence=["#f44336", "#4caf50"],
            barmode="group",
            facet_col="Group",
            facet_col_wrap=2,
            title=f"{selected_metric} Comparison",
            labels={"value": selected_metric, "variable": "Model Type"}
        )
        
        fig.update_layout(height=500)
        st.plotly_chart(fig, use_container_width=True)
        
        # æ˜¾ç¤ºå·®å¼‚ç»Ÿè®¡
        st.markdown("### Disparity Analysis")
        
        disparity_data = []
        for group in df['Group'].unique():
            group_df = df[df['Group'] == group]
            baseline_max = group_df['Baseline Model'].max()
            baseline_min = group_df['Baseline Model'].min()
            fair_max = group_df['Fair Model'].max()
            fair_min = group_df['Fair Model'].min()
            
            disparity_data.append({
                "Group": group,
                "Baseline Disparity": baseline_max - baseline_min,
                "Fair Disparity": fair_max - fair_min,
                "Improvement": ((baseline_max - baseline_min) - (fair_max - fair_min)) / (baseline_max - baseline_min) * 100
            })
        
        disparity_df = pd.DataFrame(disparity_data)
        st.dataframe(disparity_df.style.format({
            "Baseline Disparity": "{:.3f}",
            "Fair Disparity": "{:.3f}",
            "Improvement": "{:.1f}%"
        }), use_container_width=True)
        
        avg_improvement = disparity_df['Improvement'].mean()
        
        if avg_improvement > 50:
            st.markdown(f"""
            <div class="success-box">
            <h4>âœ… Strong Fairness Improvement</h4>
            <p>Average disparity reduction: <strong>{avg_improvement:.1f}%</strong></p>
            <p>The fairness-aware model significantly reduces disparities across demographic groups.</p>
            </div>
            """, unsafe_allow_html=True)
        else:
            st.markdown(f"""
            <div class="warning-box">
            <h4>âš ï¸ Moderate Fairness Improvement</h4>
            <p>Average disparity reduction: <strong>{avg_improvement:.1f}%</strong></p>
            <p>Further bias mitigation may be needed to achieve equitable outcomes.</p>
            </div>
            """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()
